{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\r\n",
      "\r\n",
      "# Copyright 2017-2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\r\n",
      "#\r\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\"). You\r\n",
      "# may not use this file except in compliance with the License. A copy of\r\n",
      "# the License is located at\r\n",
      "#\r\n",
      "#     http://aws.amazon.com/apache2.0/\r\n",
      "#\r\n",
      "# or in the \"license\" file accompanying this file. This file is\r\n",
      "# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\r\n",
      "# ANY KIND, either express or implied. See the License for the specific\r\n",
      "# language governing permissions and limitations under the License.\r\n",
      "\r\n",
      "# A sample training component that trains a simple scikit-learn decision tree model.\r\n",
      "# This implementation works in File mode and makes no assumptions about the input file names.\r\n",
      "# Input is specified as CSV with a data point in each row and the labels in the first column.\r\n",
      "\r\n",
      "from __future__ import print_function\r\n",
      "\r\n",
      "import os\r\n",
      "import json\r\n",
      "import sys\r\n",
      "import subprocess\r\n",
      "import traceback\r\n",
      "\r\n",
      "# These are the paths to where SageMaker mounts interesting things in your container.\r\n",
      "prefix = '/opt/ml/'\r\n",
      "workspace = '/workspace/bert'\r\n",
      "input_path = os.path.join(prefix,'input/data')\r\n",
      "output_path = os.path.join(prefix, 'output')\r\n",
      "model_path = os.path.join(workspace, 'bert_base.pt')\r\n",
      "param_path = os.path.join(prefix, 'input/config/hyperparameters.json')\r\n",
      "\r\n",
      "# This algorithm has a single channel of input data called 'training'. Since we run in\r\n",
      "# File mode, the input files are copied to the directory specified here.\r\n",
      "channel_name = 'training'\r\n",
      "training_path = os.path.join(input_path, channel_name)\r\n",
      "\r\n",
      "# default params\r\n",
      "training_script = 'run_squad.py'\r\n",
      "default_params = ['--init_checkpoint', str(model_path), '--do_train', '--do_lower_case', '--fp16']\r\n",
      "\r\n",
      "# Execute your training algorithm.\r\n",
      "def _run(cmd):\r\n",
      "    \"\"\"Invokes your training algorithm.\"\"\"\r\n",
      "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=os.environ)\r\n",
      "    stdout, stderr = process.communicate()\r\n",
      "\r\n",
      "    return_code = process.poll()\r\n",
      "    if return_code:\r\n",
      "        error_msg = 'Return Code: {}, CMD: {}, Err: {}'.format(return_code, cmd, stderr)\r\n",
      "        raise Exception(error_msg)\r\n",
      "\r\n",
      "\r\n",
      "def _hyperparameters_to_cmd_args(hyperparameters):\r\n",
      "    \"\"\"\r\n",
      "    Converts our hyperparameters, in json format, into key-value pair suitable for passing to our training\r\n",
      "    algorithm.\r\n",
      "    \"\"\"\r\n",
      "    cmd_args_list = []\r\n",
      "\r\n",
      "    for key, value in hyperparameters.items():\r\n",
      "        cmd_args_list.append('--{}'.format(key))\r\n",
      "        cmd_args_list.append(value)\r\n",
      "\r\n",
      "    return cmd_args_list\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    try:\r\n",
      "        # Amazon SageMaker makes our specified hyperparameters available within the\r\n",
      "        # /opt/ml/input/config/hyperparameters.json.\r\n",
      "        # https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container\r\n",
      "        #try:\r\n",
      "        os.system('cd /workspace/bert/data/squad/ && bash squad_download.sh')\r\n",
      "        with open(param_path, 'r') as tc:\r\n",
      "            hyperparams = json.load(tc)\r\n",
      "           \r\n",
      "        print(hyperparams)\r\n",
      "        python_executable = sys.executable\r\n",
      "        num_gpus = hyperparams['num_gpus']\r\n",
      "        sage_params = {}\r\n",
      "        training_params = {}\r\n",
      "        for t in hyperparams:\r\n",
      "            if(t[:4]=='sage'):\r\n",
      "                sage_params[t] = hyperparams[t]\r\n",
      "            else:\r\n",
      "                training_params[t] = hyperparams[t]\r\n",
      "        \r\n",
      "        if(int(training_params['num_gpus'])>1):\r\n",
      "            del training_params['num_gpus']\r\n",
      "            cmd_args = _hyperparameters_to_cmd_args(training_params)\r\n",
      "            train_cmd = [python_executable,'-m', 'torch.distributed.launch', f'--nproc_per_node={num_gpus}', training_script] + default_params + cmd_args\r\n",
      "        else:\r\n",
      "            del training_params['num_gpus']\r\n",
      "            cmd_args = _hyperparameters_to_cmd_args(training_params)\r\n",
      "            train_cmd = [python_executable, training_script] + default_params + cmd_args #+ sage_params['sagemaker_submit_directory']\r\n",
      "        \r\n",
      "        print('train_cmd: ',train_cmd)\r\n",
      "\r\n",
      "        #_run(train_cmd)\r\n",
      "        os.system(\" \".join(train_cmd))\r\n",
      "        \r\n",
      "        print('Training complete.')\r\n",
      "        #except:\r\n",
      "            #os.system('bash scripts/run_squad.sh')\r\n",
      "        # A zero exit code causes the job to be marked a Succeeded.\r\n",
      "        sys.exit(0)\r\n",
      "    except Exception as e:\r\n",
      "        # Write out an error file. This will be returned as the failureReason in the\r\n",
      "        # DescribeTrainingJob result.\r\n",
      "        trc = traceback.format_exc()\r\n",
      "        with open(os.path.join(output_path, 'failure'), 'w') as s:\r\n",
      "            s.write('Exception during training: ' + str(e) + '\\n' + trc)\r\n",
      "        # Printing this causes the exception to be in the training job logs, as well.\r\n",
      "        print('Exception during training: ' + str(e) + '\\n' + trc, file=sys.stderr)\r\n",
      "        # A non-zero exit code causes the training job to be marked as Failed.\r\n",
      "        sys.exit(255)\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base.pt\t\t\t   hyperparameters.json  serve\r\n",
      "bert_config.json\t\t   modeling.py\t\t tokenization.py\r\n",
      "bert_pytorch_finetuning.ipynb\t   nginx.conf\t\t train\r\n",
      "bert_pytorch_ngc_finetuning.ipynb  optimization.py\t transform_script.py\r\n",
      "bert.tar.gz\t\t\t   predictor.py\t\t Untitled.ipynb\r\n",
      "data\t\t\t\t   __pycache__\t\t utils.py\r\n",
      "Dockerfile\t\t\t   run_squad.py\t\t vocab\r\n",
      "file_utils.py\t\t\t   s3_bucket.txt\t wsgi.py\r\n",
      "helper_funcs.py\t\t\t   schedulers.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
